# VLM: Vision-Language Model
This project is a collection of interactive Jupyter notebooks demonstrating the use of state-of-the-art Vision-Language Models (VLMs), including Qwen 2.5-VL and CLIP. These notebooks provide practical examples for image classification, image captioning and object detection, making it easy to explore and experiment with cutting-edge AI models.
### Features
- Image Classification: Use CLIP to classify images based on textual prompts.
- Image Captioning: Generate natural language descriptions for images.
- Object Detection: Detect and localize objects in images using VLMs.
- Interactive Notebooks: Step-by-step guides with code, explanations, and visualizations.